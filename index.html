<!doctype html>
<html>

<head>
<title>Kun Yuan</title>
<link rel="icon" type="image/webp" href="imgs/icon.webp">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Kun Yuan, Kuaishou Technology, Artificial Intelligence, Computer Vision, 袁坤, 快手科技"> 
<meta name="description" content="Kun Yuan's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-137722442-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        // var e = document.getElementById(id);
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }

    function toggle_research_vis(target, tabElement) {
        var is_current_summary = 0;

        var e = document.getElementsByClassName("research_summary");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                if (e[i].style.display == "inline") {
                    is_current_summary = 1;
                } else {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("progress_button");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target + "_button") {
                if (is_current_summary == 0) {
                    e[i].style.display = "inline";
                }
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("goal_tabs");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id != target + "_goal_tabs") {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_research_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "research_tab";
        }
        tabElement.className = "highlight_research_tab"
    }

    function toggle_goal_vis(id, goal_tabs_id, goal) {
        var e = document.getElementById(id);
        e.style.display = "none";
        var goal_tabs = document.getElementById(goal_tabs_id);
        goal_tabs.style.display = "inline";

        var goal_tab = document.getElementById(goal + "_tab");
        toggle_goal_progress_vis(goal_tab);
        // add_goal_progress_div(goal);
    }

    function toggle_goal_progress_vis(tabElement) {
        var target = tabElement.id;
        target = target.substring(0, target.length - 4);

        var e = document.getElementsByClassName("goal_progress");
        for (var i = 0; i < e.length; i++) {
            if (e[i].id == target) {
                e[i].style.display = "inline";
            } else {
                e[i].style.display = "none";
            }
        }

        var e = document.getElementsByClassName("highlight_goal_tab");
        for (var i = 0; i < e.length; i++) {
            e[i].className = "goal_tab";
        }
        tabElement.className = "highlight_goal_tab"

        add_goal_progress_div(target);
    }

    function add_goal_progress_div(goal) {
        var e = document.getElementById(goal);
        if (e && e.children.length == 0) {
            var children = Array.from(document.getElementsByClassName(goal));
            for (var i = 0; i < children.length; i++) {
                var cloned_div = children[i].cloneNode(true);
                cloned_div.className = "publication_container";
                e.appendChild(cloned_div);
            }
        }
    }
</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Kun Yuan (袁坤)<h1>
				</div>

                <h3 class="title">R&D Expert in Artificial Intelligence and Computer Vision</h3> </h1>

				<p>
                    
                    Video Technology Group, Kuaishou Technology</br></br>

		        <b>Research Interests: </b> Visual Content Generation, Video Quality Assessment, Video Enhancement and Restoration, AI Infrastructure</br>
                    Email: <a href="mailto:yuankunbupt@gmail.com">yuankunbupt at gmail dot com </a></br>
					</br>
                    [<a href="https://scholar.google.com/citations?user=fCeZ32EAAAAJ&hl=zh-CN">Google Scholar</a>]
                    [<a href="https://www.linkedin.com/in/%E5%9D%A4-%E8%A2%81-166962182/">Linkedin</a>]
                    </br> 
				</p>

			</td>
			<td width="25%">
				<img src="photo/KunYuan-SF.jpg" width="80%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>Short Bio</h2> 

<div style="display: flex; margin-bottom: -10px">
    <p>
	    I am currently a Research & Development Expert at <a href="https://www.kuaishou.com/en/">Kuaishou Technology</a> since 2021. 
        I am committed to analyzing and improving the quality of Kuaishou videos and enhancing users' experience in live and on-demand scenarios.
        Before joining Kuaishou, I worked in <a href="https://www.sensetime.com/en">SenseTime Research</a> as a Computer Vision and Machine Learning Researcher from 2018 to 2021, 
        improving the accuracy of face recognition and classification in smart city scenarios.
        I received my master degree from the National Laboratory of Pattern Recognition (NLPR), <a href="https://www.sensetime.com/en">Institute of Automation</a>, Chinese Academy of Science in 2018, 
        and my bachelor degree from the <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications (BUPT)</a> in 2015. 
    </br></br>
        My research interests are in visual content generation, video quality assessment, video enhancement and restoration, neural architecture design and AI infrastructure.
		</br>
		<!--<b>I'd like to connect with anyone who's passionate about research. Excited for some great intellectual discussions!</b>-->
    </p>
</div>

<h2>工作介绍</h2>

<div style="display: flex; margin-bottom: -10px">
    <p style="font-size: 14px;">
        自2021年加入快手，主要深耕于人工智能与音视频领域的结合落地，专注于通过算法提升快手视频的整体画质并改善用户体验：
    </br></br>
        <b>1. 视频质量评估：</b>基于海量的视频数据+AI大模型训练自研了 <a href="https://mp.weixin.qq.com/s/8rf-Bm918oCF6OGKTuGEUg">快手视频质量评价体系KVQ</a>，量化视频生产消费链路中诸如编码、处理、传输等过程的画质损失，提供准确的客观质量评价。
        通过自研 <a href="https://mp.weixin.qq.com/s/qvbwgfcJmO0x_uCV4fMHsQ">QPT系列算法</a>，走通了基于海量无监督数据训练质量感知模型的技术路线，结合高质垂类数据微调，在快手100+垂类场景的表现超过Golden Eye；
        并与多模态大模型进行结合，通过高质量描述数据指令微调，给出白盒化归因分析和画质改善建议。落地快手点播、直播场景，指导智能编码、多码率决策下发、审核风控、推荐分发、搜索排序等场景，日均调用2亿次。
    </br></br>
        <b>2. 视频画质增强：</b>针对快手视频的画质问题，基于Transformer设计并实现了多种视频处理算法，包括KEP (Kuaishou Enhanced Processing)/KRP (Kuaishou Restoration Processing)，显著改善了视频画质，让用户看到比作者上传源更清晰的画面，
        取得了显著的带宽成本节省和App使用时长提升收益。充分拥抱AIGC，进一步自研Diffusion-based <a href="https://mp.weixin.qq.com/s/glv8eavgDlbf6y_3r-kuPg">XPSR</a> 和
        Autoregressive-based <a href="https://mp.weixin.qq.com/s/TpNHkQk__tnmf88yNsYnLQ">VARSR</a> 的生成式大模型，通过生成能力的改善突破画质上限，结合billion级别的训练数据，取得了令人惊艳的增强修复效果，落地服务端点播场景取得了显著用户时长提升收益，
        同时赋能电商、商业化，通过清晰度的提升促进GMV、广告消耗。
    </p>
</div>

<h2>News</h2>

<ul>
    <li>
        <div class="marker">[2025-05] One paper accepted by ICML 2025.</div>
    </li>
    <li>
        <div class="marker">[2025-03] I give a talk at Nvidia GTC 2025 about "Redefining Visual Experience of Short-form Videos: Accelerating Large Models for Intelligent Video Quality Assessment and Processing by TensorRT-LLM".</div>
    </li>
    <li>
        <div class="marker">[2025-03] One paper accepted by CVPR 2025.</div>
    </li>
    <li>
        <div class="marker">[2024-07] Two papers accepted by ACM MM 2024.</div>
    </li>
	<li>
        <div class="marker">[2024-07] One paper accepted by ECCV 2024.</div>
    </li>
    <li>
        <div class="marker">[2024-03] Two papers accepted by CVPR 2024.</div>
    </li>
    <li>
        <div class="marker">[2023-10] Two papers accepted by ACM MM 2023.</div>
    </li>
    <li>
        <div class="marker">[2023-03] One paper accepted by CVPR 2023.</div>
    </li>
    <li>
        <div class="marker">[2022-03] One paper accepted by CVPR 2022.</div>
    </li>
    <li>
        <div class="marker">[2021-02] One paper accepted by ICLR 2021.</div>
    </li>
    <li>
        <div class="marker">[2021-02] Two papers accepted by ICCV 2021.</div>
    </li>
    <li>
        <div class="marker">[2020-08] One paper accepted by ECCV 2020.</div>
    </li>
    <li>
        <div class="marker">[2018-07] One paper accepted by IJCAI 2018.</div>
    </li>
</ul>

<div class="show_button">
    <a href="javascript:toggle_vis('news')" id="showText">[Show more]</a>
</div>

<h2>
    Publications
    <span style="font-size: 50%;">(* denotes equal contribution, # denotes corresponding author)</span>
</h2>

<div class="newline_bg">
    <h3>2025</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICML2025/VARSR.png">
    </div>
    <div class="publication_title">
        Visual Autoregressive Modeling for Image Super-Resolution</br>
        Yunpeng Qu, <b>Kun Yuan#</b>, Jinhua Hao, Kai Zhao, Qizhi Xie, Ming Sun, Chao Zhou</br>
	    International Conference on Machine Learning (ICML), 2025.</br>
        [<a href="https://arxiv.org/pdf/2501.18993">Paper</a>][<a href="https://github.com/quyp2000/VARSR">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2025/kvq.jpg">
    </div>
    <div class="publication_title">
        KVQ: Boosting Video Quality Assessment via Saliency-guided Local Perception</br>
        Yunpeng Qu, <b>Kun Yuan#</b>, Qizhi Xie, Ming Sun, Chao Zhou, Jian Wang</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025.</br>
        [<a href="https://arxiv.org/pdf/2503.10259">Paper</a>][<a href="https://github.com/qyp2000/KVQ">Project Page</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2024</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ACMMM2024/qptv2.png">
    </div>
    <div class="publication_title">
        QPT V2: Masked Image Modeling Advances Visual Scoring</br>
        Qizhi Xie, <b>Kun Yuan#</b>, Yunpeng Qu, Mingda Wu, Ming Sun, Chao Zhou, Jihong Zhu</br>
	    ACM International Conference on Multimedia (ACM MM), 2024.</br>
        [<a href="https://arxiv.org/pdf/2407.16541">Paper</a>][<a href="https://github.com/KeiChiTse/QPT-V2">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ACMMM2024/qncd.png">
    </div>
    <div class="publication_title">
        QNCD: Quantization Noise Correction for Diffusion Models</br>
        Huanpeng Chu, Wei Wu, Chengjie Zang, <b>Kun Yuan</b></br>
	    ACM International Conference on Multimedia (ACM MM), 2024.</br>
        [<a href="https://arxiv.org/pdf/2403.19140">Paper</a>][<a href="https://github.com/huanpengchu/QNCD">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ECCV2024/xpsr.png">
    </div>
    <div class="publication_title">
        XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</br>
        Yunpeng Qu*, <b>Kun Yuan*</b>, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</br>
	    European Conference on Computer Vision (ECCV), 2024.</br>
        [<a href="https://arxiv.org/abs/2403.05049">Paper</a>][<a href="https://github.com/qyp2000/XPSR">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2024/kvq.png">
    </div>
    <div class="publication_title">
        KVQ: Kwai Video Quality Assessment for Short-form Videos</br>
        Yiting Lu*, Xin Li*, Yajing Pei*, <b>Kun Yuan#</b>, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen#</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_KVQ_Kwai_Video_Quality_Assessment_for_Short-form_Videos_CVPR_2024_paper.pdf">Paper</a>]
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lu_KVQ_Kwai_Video_CVPR_2024_supplemental.pdf">Supp</a>]
        [<a href="https://lixinustc.github.io/projects/KVQ">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2024/ptmvqa.png">
    </div>
    <div class="publication_title">
        PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild</br>
        <b>Kun Yuan*</b>, Hongbo Liu*, Mading Li*, Muyi Sun, Ming Sun, Jiachao Gong, Jinhua Hao, Chao Zhou, Yansong Tang</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_PTM-VQA_Efficient_Video_Quality_Assessment_Leveraging_Diverse_PreTrained_Models_from_CVPR_2024_paper.pdf">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2023</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ACMMM2023/vqt.png">
    </div>
    <div class="publication_title">
        Capturing Co-existing Distortions in User-Generated Content for No-reference Video Quality Assessment</br>
        <b>Kun Yuan*</b>, Zishang Kong*, Chuanchuan Zheng, Ming Sun, Xing Wen</br>
	    ACM International Conference on Multimedia (ACM MM), 2023.</br>
        [<a href="https://arxiv.org/pdf/2307.16813">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ACMMM2023/adadqa.png">
    </div>
    <div class="publication_title">
        Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment</br>
        Hongbo Liu*, Mingda Wu*, <b>Kun Yuan*</b>, Ming Sun, Yansong Tang, Chuanchuan Zheng, Xing Wen, Xiu Li</br>
	    ACM International Conference on Multimedia (ACM MM), 2023.</br>
        [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611795">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2023/qpt.png">
    </div>
    <div class="publication_title">
        Quality-aware Pre-trained Models for Blind Image Quality Assessment</br>
        Kai Zhao*, <b>Kun Yuan*</b>, Ming Sun, Mading Li, Xing Wen</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Quality-Aware_Pre-Trained_Models_for_Blind_Image_Quality_Assessment_CVPR_2023_paper.pdf">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2022</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/BMVC2022/showface.png">
    </div>
    <div class="publication_title">
        ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks</br>
        Zhuojie Wu, Xingqun Qi, Zijian Wang, Wanting Zhou, <b>Kun Yuan</b>, Muyi Sun, Zhenan Sun</br>
	    British Machine Vision Conference (BMVC), 2022.</br>
        [<a href="https://bmvc2022.mpi-inf.mpg.de/0052.pdf">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2022/scm.png">
    </div>
    <div class="publication_title">
        Self-supervised Correlation Mining Network for Person Image Generation</br>
        Zijian Wang, Xingqun Qi, <b>Kun Yuan</b>, Muyi Sun</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Self-Supervised_Correlation_Mining_Network_for_Person_Image_Generation_CVPR_2022_paper.pdf">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>2021</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICLR2021/nm.png">
    </div>
    <div class="publication_title">
        Learning N:M Fine-grained Structured Sparse Neural Networks from Scratch</br>
        Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, <b>Kun Yuan</b>, Wenxiu Sun, Hongsheng Li</br>
	    International Conference on Learning Representations (ICLR), 2021.</br>
        [<a href="https://openreview.net/pdf?id=K9bw7vqp_s">Paper</a>]
        [<a href="https://github.com/aojunzz/NM-sparsity">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICCV2021/ceit.png">
    </div>
    <div class="publication_title">
        Incorporating Convolution Designs into Visual Transformers</br>
        <b>Kun Yuan</b>, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu</br>
	    IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</br>
        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Incorporating_Convolution_Designs_Into_Visual_Transformers_ICCV_2021_paper.pdf">Paper</a>]
        [<a href="https://github.com/coeusguo/ceit">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICCV2021/ddw.png">
    </div>
    <div class="publication_title">
        Differentiable Dynamic Wirings for Neural Networks</br>
        <b>Kun Yuan</b>, Quanquan Li, Shaopeng Guo, Dapeng Chen, Aojun Zhou, Fengwei Yu, Ziwei Liu</br>
	    IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</br>
        [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yuan_Differentiable_Dynamic_Wirings_for_Neural_Networks_ICCV_2021_paper.pdf">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>Earlier</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ECCV2020/topo.png">
    </div>
    <div class="publication_title">
        Learning Connectivity of Neural Networks from a Topological Perspective</br>
        <b>Kun Yuan</b>, Quanquan Li, Jing Shao, Junjie Yan</br>
	    European Conference on Computer Vision (ECCV), 2020.</br>
        [<a href="https://arxiv.org/pdf/2008.08261">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/IJCAI2018/safenet.png">
    </div>
    <div class="publication_title">
        SafeNet: Scale-normalization and Anchor-based Feature Extraction Network for Person Re-identification</br>
        <b>Kun Yuan</b>, Qian Zhang, Chang Huang, Shiming Xiang, Chunhong Pan</br>
	    International Joint Conferences on Artificial Intelligence (IJCAI), 2018.</br>
        [<a href="https://www.ijcai.org/proceedings/2018/0156.pdf">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICDAR2017/seg.png">
    </div>
    <div class="publication_title">
        Deep Networks for Degraded Document Image Binarization through Pyramid Reconstruction</br>
        Gaofeng Meng, <b>Kun Yuan</b>, Ying Wu, Shiming Xiang, Chunhong Pan</br>
	    International Conference on Document Analysis and Recognition (ICDAR), 2017.</br>
        [<a href="https://ieeexplore.ieee.org/abstract/document/8270055">Paper</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/ICIP2017/fcn.png">
    </div>
    <div class="publication_title">
        Efficient Cloud Detection in Remote Sensing Images using Edge-aware Segmentation Network and Easy-to-hard Training Strategy</br>
        <b>Kun Yuan</b>, Gaofeng Meng, Dongcai Cheng, Jun Bai, Shiming Xiang, Chunhong Pan</br>
	    IEEE International Conference on Image Processing (ICIP), 2017.</br>
        [<a href="https://ieeexplore.ieee.org/abstract/document/8296243">Paper</a>]
    </div>
</div>

<div class="newline_bg">
    <h3>Workshops</h3>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2024/comp.png">
    </div>
    <div class="publication_title">
        NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods and Results</br>
        Xin Li, <b>Kun Yuan</b>, Yajing Pei, Yiting Lu, Ming Sun, Chao Zhou, Zhibo Chen, Radu Timofte</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2024.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Li_NTIRE_2024_Challenge_on_Short-form_UGC_Video_Quality_Assessment_Methods_CVPRW_2024_paper.pdf">Paper</a>]
        [<a href="https://github.com/lixinustc/KVQ-Challenge-CVPR-NTIRE2024">Project Page</a>]
    </div>
</div>

<div class="publication_container scene_generation">
    <div class="publication_image">
        <img src="paper_teaser/CVPR2024/zoom.png">
    </div>
    <div class="publication_title">
        Zoom-VQA: Patches, Frames and Clips Integration for Video Quality Assessment</br>
        Kai Zhao, <b>Kun Yuan</b>, Ming Sun, Xing Wen</br>
	    IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), 2023.</br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhao_Zoom-VQA_Patches_Frames_and_Clips_Integration_for_Video_Quality_Assessment_CVPRW_2023_paper.pdf">Paper</a>]
        [<a href="https://github.com/k-zha14/Zoom-VQA">Project Page</a>]
    </div>
</div>

<h2>Awards</h2>

<ul>
    <li>                                                                        
        <div class="marker">快手研发线优秀项目奖:“基于Transformer的视频处理模型研究与落地”</div> <div>2024</div>
    </li>                                                                   
    <li>                                                                        
        <div class="marker">快手洛子峰奖:“KVQ:基于 AI 的视频质量评价”</div> <div>2023</div>
    </li>
    <li>                                                                        
        <div class="marker">快手洛子峰奖:“基于主观的智能视频增强与编解码架构联合优化”</div> <div>2023</div>
    </li>                                                                  
</ul>

</div>
</body>
</html>
